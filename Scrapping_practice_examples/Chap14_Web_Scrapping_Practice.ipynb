{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'www.naver.com'\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "naver_search_url = \"http://search.naver.com/search.naver?query=\"\n",
    "search_word = \"파이썬\"\n",
    "\n",
    "url = naver_search_url + search_word\n",
    "\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "google_url = \"www.google.com/#q=\"\n",
    "search_word = 'python'\n",
    "url = google_url + search_word\n",
    "\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개의 웹 사이트 접속하기\n",
    "import webbrowser\n",
    "\n",
    "urls = [\"www.naver.com\", \"www.daum.net\", \"www.google.com\"]\n",
    "\n",
    "for url in urls:\n",
    "    webbrowser.open_new(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글에서 여러 개의 검색어를 입력해 결과를 얻는 코드\n",
    "import webbrowser\n",
    "\n",
    "google_url = \"www.google.com/#q=\"\n",
    "search_words = ['python web scraping', 'python webbrowser']\n",
    "\n",
    "for search_word in search_words:\n",
    "    webbrowser.open_new(google_url + search_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Dropbox\\\\데이터_분석을_위한_파이썬_철저_입문\\\\chap14'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\HTML_example.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\HTML_example.html\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <meta charset=\"utf-8\">\n",
    "            <title>이것은 HTML 예제</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>출간된 책 정보</h1>\n",
    "            <p id=\"book_title\">이해가 쏙쏙 되는 파이썬</p>\n",
    "            <p id=\"author\">홍길동</p>\n",
    "            <p id=\"publisher\">위키북스 출판사</p>\n",
    "            <p id=\"year\">2018</p>\n",
    "        </body>\n",
    "    </html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\HTML_example2.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\HTML_example2.html\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <meta charset=\"utf-8\">\n",
    "            <title>이것은 HTML 예제</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>출간된 책 정보</h1>\n",
    "            <p>이해가 쏙쏙 되는 파이썬</p>\n",
    "            <p>홍길동</p>\n",
    "            <p>위키북스 출판사</p>\n",
    "            <p>2018</p>\n",
    "        </body>\n",
    "    </html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTML 소스코드 스크래핑 기본 예제\n",
    "import requests\n",
    "\n",
    "r = requests.get(\"https://www.google.co.kr\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content=\"text/html; charset=UTF-8\" http-equiv=\"Content-Type\"><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><title>Google</title><script nonce=\"BniNqjvR2BDf7gdTM8VXAg==\">(function(){window.google={kEI:\\'DDPZXNuKJ5KFr7wPhsCpUA\\',kEXPI:\\'0,1353747,57,1957,2423,698,527,590,141,223,1575,1258,1893,379,206,791,226,166,9,430,31,69,30,288,19,2332580,329502,1294,12383,4855,32691,15248,867,2236,10221,4987,1954,9286,369,4577,4242,2442,260,4204,903,575,835,284,2,579,727,2431,1362,4323,4968,773,2253,5892,2,1965,2595,3601,669,1050,1808,1397,81,7,491,620,29,1395,978,7931,1288,2,3106,901,796,1220,38,919,747,8,119,1217,1364,346,1,1264,2729,7,1826,1235,2,631,2562,2,4,2,670,44,4148,510,125,1160,1447,632,2228,655,21,89,19,209,1593,389,144,1245,777,1,2,366,1017,300,705,758,96,392,30,229,172,374,2,614,705,402,10,168,8,109,187,831,235,292,151,367,453,171,563,250,154,48,553,11,14,12,571,356,39,693,650,187,207,767,9,25,177,168,155,5,1165,87,67,90,141,29,214,299,324,193,110,421,278,93,86,83,104,25,227,57,38,158,6,439,1454,331,68,15,10,638,299,22,818,260,123,538,528,2,7,7,187,316,644,11,337,512,42,606,635,3,128,12,17,174,332,205,1,206,118,39,170,346,740,606,2,844,98,254,55,588,14,205,92,5936031,2920,5997515,41,2799864,4,1572,549,333,444,1,2,80,1,900,579,9,308,1,8,1,2,2132,1,1,1,1,1,414,1,748,141,59,726,3,7,563,1,2047,28,6,2,9,1,14,17,1,14,5,4,18\\',authuser:0,kscs:\\'c9c918f0_DDPZXNuKJ5KFr7wPhsCpUA\\',kGL:\\'KR\\'};google.sn=\\'webhp\\';google.kHL=\\'ko\\';})();(function(){google.lc=[];google.li=0;google.getEI=function(a){for(var b;a&&(!a.getAttribute||!(b=a.getAttribute(\"eid\")));)a=a.parentNode;return b||google.kEI};google.getLEI=function(a){for(var b=null;a&&(!a.getAttribute||!(b=a.getAttribute(\"leid\")));)a=a.parentNode;return b};google.https=function(){return\"https:\"==window.location.protocol};google.ml=function(){return null};google.time=function(){return(new Date).getTime()};google.log=function(a,b,e,c,g){if(a=google.logUrl(a,b,e,c,g)){b=new Image;var d=google.lc,f=google.li;d[f]=b;b.onerror=b.onload=b.onabort=function(){delete d[f]};google.vel&&google.vel.lu&&google.vel.lu(a);b.src=a;google.li=f+1}};google.logUrl=function(a,b,e,c,g){var d=\"\",f=google.ls||\"\";e||-1!=b.search(\"&ei=\")||(d=\"&ei=\"+google.getEI(c),-1==b.search(\"&lei=\")&&(c=google.getLEI(c))&&(d+=\"&lei=\"+c));c=\"\";!e&&google.cshid&&-1==b.search(\"&cshid=\")&&\"slh\"!=a&&(c=\"&cshid=\"+google.cshid);a=e||\"/\"+(g||\"gen_204\")+\"?atyp=i&ct=\"+a+\"&cad=\"+b+d+f+\"&zx=\"+google.time()+c;/^http:/i.test(a)&&google.https()&&(google.ml(Error(\"a\"),!1,{src:a,glmm:1}),a=\"\");return a};}).call(this);(function(){google.y={};google.x=function(a,b){if(a)var c=a.id;else{do c=Math.random();while(google.y[c])}google.y[c]=[a,b];return!1};google.lm=[];google.plm=function(a){google.lm.push.apply(google.lm,a)};google.lq=[];google.load=function(a,b,c){google.lq.push([[a],b,c])};google.loadAll=function(a,b){google.lq.push([a,b])};}).call(this);google.f={};var a=window.location,b=a.href.indexOf(\"#\");if(0<=b){var c=a.href.substring(b+1);/(^|&)q=/.test(c)&&-1==c.indexOf(\"#\")&&a.replace(\"/search?\"+c.replace(/(^|&)fp=[^&]*/g,\"\")+\"&cad=h\")};</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}\\n</style><style>body,td,a,p,.h{font-family:굴림,돋움,arial,sans-serif}.ko{font-size:9pt}body{margin:0;overflow-y:scroll}#gog{padding:3px 8px 0}td{line-height:.8em}.gac_m td{line-height:17px}form{margin-bottom:20px}.h{color:#36c}.q{color:#00c}.ts td{padding:0}.ts{border-collapse:collapse}em{font-weight:bold;font-style:normal}.lst{height:25px;width:496px}.gsfi,.lst{font:18px arial,sans-serif}.gsfs{font:17px arial,sans-serif}.ds{display:inline-box;display:inline-block;margin:3px 0 4px;margin-left:4px}input{font-family:inherit}a.gb1,a.gb2,a.gb3,a.gb4{color:#11c !important}body{background:#fff;color:black}a{color:#11c;text-decoration:none}a:hover,a:active{text-decoration:underline}.fl a{color:#36c}a:visited{color:#551a8b}a.gb1,a.gb4{text-decoration:underline}a.gb3:hover{text-decoration:none}#ghead a.gb2:hover{color:#fff !important}.sblc{padding-top:5px}.sblc a{display:block;margin:2px 0;margin-left:13px;font-size:11px}.lsbb{background:#eee;border:solid 1px;border-color:#ccc #999 #999 #ccc;height:30px}.lsbb{display:block}.ftl,#fll a{display:inline-block;margin:0 12px}.lsb{background:url(/images/nav_logo229.png) 0 -261px repeat-x;border:none;color:#000;cursor:pointer;height:30px;margin:0;outline:0;font:15px arial,sans-serif;vertical-align:top}.lsb:active{background:#ccc}.lst:focus{outline:none}.tiah{width:458px}</style><script nonce=\"BniNqjvR2BDf7gdTM8VXAg==\"></script></head><body bgcolor=\"#fff\"><script nonce=\"BniNqjvR2BDf7gdTM8VXAg==\">(function(){var src=\\'/images/nav_logo229.png\\';var iesg=false;document.body.onload = function(){window.n && window.n();if (document.images){new Image().src=src;}\\nif (!iesg){document.f&&document.f.q.focus();document.gbqf&&document.gbqf.q.focus();}\\n}\\n})();</script><div id=\"mngb\"> <div id=gbar><nobr><b class=gb1>검색</b> <a class=gb1 href=\"https://www.google.co.kr/imghp?hl=ko&tab=wi\">이미지</a> <a class=gb1 href=\"https://maps.google.co.kr/maps?hl=ko&tab=wl\">지도</a> <a class=gb1 href=\"https://play.google.com/?hl=ko&tab=w8\">Play</a> <a class=gb1 href=\"https://www.youtube.com/?gl=KR&tab=w1\">YouTube</a> <a class=gb1 href=\"https://news.google.co.kr/nwshp?hl=ko&tab=wn\">뉴스</a> <a class=gb1 href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a> <a class=gb1 href=\"https://drive.google.com/?tab=wo\">드라이브</a> <a class=gb1 style=\"text-decoration:none\" href=\"https://www.google.co.kr/intl/ko/about/products?tab=wh\"><u>더보기</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a href=\"http://www.google.co.kr/history/optout?hl=ko\" class=gb4>웹 기록</a> | <a  href=\"/preferences?hl=ko\" class=gb4>설정</a> | <a target=_top id=gb_70 href=\"https://accounts.google.com/ServiceLogin?hl=ko&passive=true&continue=https://www.google.co.kr/\" class=gb4>로그인</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div> </div><center><br clear=\"all\" id=\"lgpd\"><div id=\"lga\"><img alt=\"Google\" height=\"92\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\" id=\"hplogo\" onload=\"window.lol&&lol()\"><br><br></div><form action=\"/search\" name=\"f\"><table cellpadding=\"0\" cellspacing=\"0\"><tr valign=\"top\"><td width=\"25%\">&nbsp;</td><td align=\"center\" nowrap=\"\"><input name=\"ie\" value=\"EUC-KR\" type=\"hidden\"><input value=\"ko\" name=\"hl\" type=\"hidden\"><input name=\"source\" type=\"hidden\" value=\"hp\"><input name=\"biw\" type=\"hidden\"><input name=\"bih\" type=\"hidden\"><div class=\"ds\" style=\"height:32px;margin:4px 0\"><div style=\"position:relative;zoom:1\"><input style=\"color:#000;margin:0;padding:5px 8px 0 6px;vertical-align:top;padding-right:38px\" autocomplete=\"off\" class=\"lst tiah\" value=\"\" title=\"Google 검색\" maxlength=\"2048\" name=\"q\" size=\"57\"><img src=\"/textinputassistant/tia.png\" style=\"position:absolute;cursor:pointer;right:5px;top:4px;z-index:300\" data-script-url=\"/textinputassistant/11/ko_tia.js\" alt=\"\" height=\"23\" onclick=\"var s=document.createElement(\\'script\\');s.src=this.getAttribute(\\'data-script-url\\');(document.getElementById(\\'xjsc\\')||document.body).appendChild(s);\" width=\"27\"></div></div><br style=\"line-height:0\"><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" value=\"Google 검색\" name=\"btnG\" type=\"submit\"></span></span><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" value=\"I’m Feeling Lucky\" name=\"btnI\" onclick=\"if(this.form.q.value)this.checked=1; else top.location=\\'/doodles/\\'\" type=\"submit\"></span></span></td><td class=\"fl sblc\" align=\"left\" nowrap=\"\" width=\"25%\"><a href=\"/advanced_search?hl=ko&amp;authuser=0\">고급검색</a><a href=\"/language_tools?hl=ko&amp;authuser=0\">언어도구</a></td></tr></table><input id=\"gbv\" name=\"gbv\" type=\"hidden\" value=\"1\"><script nonce=\"BniNqjvR2BDf7gdTM8VXAg==\">(function(){var a,b=\"1\";if(document&&document.getElementById)if(\"undefined\"!=typeof XMLHttpRequest)b=\"2\";else if(\"undefined\"!=typeof ActiveXObject){var c,d,e=[\"MSXML2.XMLHTTP.6.0\",\"MSXML2.XMLHTTP.3.0\",\"MSXML2.XMLHTTP\",\"Microsoft.XMLHTTP\"];for(c=0;d=e[c++];)try{new ActiveXObject(d),b=\"2\"}catch(h){}}a=b;if(\"2\"==a&&-1==location.search.indexOf(\"&gbv=2\")){var f=google.gbvu,g=document.getElementById(\"gbv\");g&&(g.value=a);f&&window.setTimeout(function(){location.href=f},0)};}).call(this);</script></form><div id=\"gac_scont\"></div><div style=\"font-size:83%;min-height:3.5em\"><br></div><span id=\"footer\"><div style=\"font-size:10pt\"><div style=\"margin:19px auto;text-align:center\" id=\"fll\"><a href=\"/intl/ko/ads/\">광고 프로그램</a><a href=\"http://www.google.co.kr/intl/ko/services/\">비즈니스 솔루션</a><a href=\"/intl/ko/about.html\">Google 정보</a><a href=\"https://www.google.co.kr/setprefdomain?prefdom=US&amp;sig=K_t-5gAmld1A4ZC_4XmSFEVQiC8Ck%3D\" id=\"fehl\">Google.com</a></div></div><p style=\"color:#767676;font-size:8pt\">&copy; 2019 - <a href=\"/intl/ko/policies/privacy/\">개인정보처리방침</a> - <a href=\"/intl/ko/policies/terms/\">약관</a></p></span></center><script nonce=\"BniNqjvR2BDf7gdTM8VXAg==\">(function(){window.google.cdo={height:0,width:0};(function(){var a=window.innerWidth,b=window.innerHeight;if(!a||!b){var c=window.document,d=\"CSS1Compat\"==c.compatMode?c.documentElement:c.body;a=d.clientWidth;b=d.clientHeight}a&&b&&(a!=google.cdo.width||b!=google.cdo.height)&&google.log(\"\",\"\",\"/client_204?&atyp=i&biw=\"+a+\"&bih=\"+b+\"&ei=\"+google.kEI);}).call(this);})();(function(){var u=\\'/xjs/_/js/k\\\\x3dxjs.hp.en.895jBrucqC0.O/m\\\\x3dsb_he,d/am\\\\x3dYFAL/rt\\\\x3dj/d\\\\x3d1/rs\\\\x3dACT90oGKzJ7n-id1hYoy_XFc1MqmaoFGGg\\';setTimeout(function(){var a=document.createElement(\"script\");a.src=u;google.timers&&google.timers.load&&google.tick&&google.tick(\"load\",\"xjsls\");document.body.appendChild(a)},0);})();(function(){window.google.xjsu=\\'/xjs/_/js/k\\\\x3dxjs.hp.en.895jBrucqC0.O/m\\\\x3dsb_he,d/am\\\\x3dYFAL/rt\\\\x3dj/d\\\\x3d1/rs\\\\x3dACT90oGKzJ7n-id1hYoy_XFc1MqmaoFGGg\\';})();function _DumpException(e){throw e;}\\n(function(){google.spjs=false;})();google.sm=1;(function(){var pmc=\\'{\\\\x22Qnk92g\\\\x22:{},\\\\x22RWGcrA\\\\x22:{},\\\\x22U5B21g\\\\x22:{},\\\\x22YFCs/g\\\\x22:{},\\\\x22ZI/YVQ\\\\x22:{},\\\\x22d\\\\x22:{},\\\\x22sb_he\\\\x22:{\\\\x22agen\\\\x22:true,\\\\x22cgen\\\\x22:true,\\\\x22client\\\\x22:\\\\x22heirloom-hp\\\\x22,\\\\x22dh\\\\x22:true,\\\\x22dhqt\\\\x22:true,\\\\x22ds\\\\x22:\\\\x22\\\\x22,\\\\x22ffql\\\\x22:\\\\x22ko\\\\x22,\\\\x22fl\\\\x22:true,\\\\x22host\\\\x22:\\\\x22google.co.kr\\\\x22,\\\\x22isbh\\\\x22:28,\\\\x22jsonp\\\\x22:true,\\\\x22msgs\\\\x22:{\\\\x22cibl\\\\x22:\\\\x22검색어 지우기\\\\x22,\\\\x22dym\\\\x22:\\\\x22이것을 찾으셨나요?\\\\x22,\\\\x22lcky\\\\x22:\\\\x22I’m Feeling Lucky\\\\x22,\\\\x22lml\\\\x22:\\\\x22자세히 알아보기\\\\x22,\\\\x22oskt\\\\x22:\\\\x22입력 도구\\\\x22,\\\\x22psrc\\\\x22:\\\\x22검색어가 \\\\\\\\u003Ca href\\\\x3d\\\\\\\\\\\\x22/history\\\\\\\\\\\\x22\\\\\\\\u003E웹 기록\\\\\\\\u003C/a\\\\\\\\u003E에서 삭제되었습니다.\\\\x22,\\\\x22psrl\\\\x22:\\\\x22삭제\\\\x22,\\\\x22sbit\\\\x22:\\\\x22이미지로 검색\\\\x22,\\\\x22srch\\\\x22:\\\\x22Google 검색\\\\x22},\\\\x22ovr\\\\x22:{},\\\\x22pq\\\\x22:\\\\x22\\\\x22,\\\\x22refpd\\\\x22:true,\\\\x22refspre\\\\x22:true,\\\\x22rfs\\\\x22:[],\\\\x22sbpl\\\\x22:24,\\\\x22sbpr\\\\x22:24,\\\\x22scd\\\\x22:10,\\\\x22sce\\\\x22:5,\\\\x22stok\\\\x22:\\\\x22X1rtBao3QFBo4ASmr3Ubtwurg3k\\\\x22,\\\\x22uhde\\\\x22:false}}\\';google.pmc=JSON.parse(pmc);})();</script>        </body></html>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r.text[0:100]\n",
    "r.text[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이렇게 하는 것도 가능할 것이다.\n",
    "import requests\n",
    "\n",
    "html = requests.get(\"https://www.google.co.kr\").text\n",
    "html[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><div><span> <a href=\"http://www.naver.com\">naver</a> <a href=\"https://www.google.com\">google</a> <a href=\"http://www.daum.net/\">daum</a> </span></div></body></html>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beautifulsoup4 사용예제\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 테스트용 html 코드\n",
    "html = \"\"\"<html><body><div><span>\\\n",
    "        <a href=http://www.naver.com>naver</a>\\\n",
    "        <a href=https://www.google.com>google</a>\\\n",
    "        <a href=http://www.daum.net/>daum</a>\\\n",
    "        </span></div></body></html>\"\"\"\n",
    "\n",
    "# BeautifulSoup를 이용해 HTML 소스를 파싱\n",
    "# lxml : HTML 소스를 처리하기 위한 Parser\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <span>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "    <a href=\"https://www.google.com\">\n",
      "     google\n",
      "    </a>\n",
      "    <a href=\"http://www.daum.net/\">\n",
      "     daum\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://www.naver.com\">naver</a>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTML 소스코드에 있는 첫 번째 a 태그를 찾아서 반환\n",
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naver'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 태그 반환 결과에 텍스트 요소만 추리기\n",
    "soup.find('a').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.naver.com\">naver</a>,\n",
       " <a href=\"https://www.google.com\">google</a>,\n",
       " <a href=\"http://www.daum.net/\">daum</a>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n",
      "google\n",
      "daum\n"
     ]
    }
   ],
   "source": [
    "# a 태그로 엮인 사이트링크 태그에서 사이트 이름만 추출하는 예제\n",
    "site_names = soup.find_all('a')\n",
    "for site_name in site_names:\n",
    "    print(site_name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup 사용 다른 예제\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html2 = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>작품과 작가 모음</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>책 정보</h1>\n",
    "        <p id=\"book_title\">토지</p>\n",
    "        <p id=\"author\">박경리</p>\n",
    "        \n",
    "        <p id=\"book_title\">태백산맥</p>\n",
    "        <p id=\"author\">조정래</p>\n",
    "        \n",
    "        <p id=\"book_title\">감옥으로부터의 사색</p>\n",
    "        <p id=\"author\">신영복</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>작품과 작가 모음</title>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# title 태그만 추려봅니다\n",
    "soup2.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<h1>책 정보</h1>\n",
       "<p id=\"book_title\">토지</p>\n",
       "<p id=\"author\">박경리</p>\n",
       "<p id=\"book_title\">태백산맥</p>\n",
       "<p id=\"author\">조정래</p>\n",
       "<p id=\"book_title\">감옥으로부터의 사색</p>\n",
       "<p id=\"author\">신영복</p>\n",
       "</body>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# body 태그만 추려봅니다\n",
    "soup2.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>책 정보</h1>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# body에 있는 h1 태그만 추려봄\n",
    "soup2.body.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 책 제목과 작가를 구분하지 않고 p 태그가 있는 요소를 모두 가지고 온 것을 볼 수 있다.\n",
    "# find(_all)은 사실 이렇게 생겨먹었다.\n",
    "# BeautifulSoup.find_all('태그', '속성')\n",
    "# BeautifulSoup.find('태그', '속성')\n",
    "soup2.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"book_title\">토지</p>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find 함수에 두번째 parameter 사용 예제\n",
    "soup2.find('p', {\"id\": \"book_title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"author\">박경리</p>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id : author\n",
    "soup2.find('p', {\"id\": \"author\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p', {\"id\": \"book_title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p', {\"id\": \"author\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토지/박경리\n",
      "태백산맥/조정래\n",
      "감옥으로부터의 사색/신영복\n"
     ]
    }
   ],
   "source": [
    "# 이를 바탕으로 책 제목과, 작가만 추리는 예제\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")\n",
    "\n",
    "book_titles = soup2.find_all('p', {\"id\": \"book_title\"})\n",
    "authors = soup2.find_all('p', {\"id\": \"author\"})\n",
    "\n",
    "for book_title, author in zip(book_titles, authors):\n",
    "    print(book_title.get_text() + \"/\" + author.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>책 정보</h1>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# css 선택자를 이용한 예제\n",
    "soup2.select('body h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('body p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p 태그가 유일하게 있을경우 이렇게 축약도 된다\n",
    "soup2.select('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id 속성을 추출하는 다른 방법\n",
    "soup2.select('p#book_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('p#author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\HTML_example_my_site.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\HTML_example_my_site.html\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <meta charset=\"utf-8\">\n",
    "            <title>사이트 모음</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <p id=\"title\"><b>자주 가는 사이트 모음</b></p>\n",
    "            <p id=\"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>\n",
    "            <a href=\"http://www.naver.com\" class=\"portal\" id=\"naver\">네이버</a> <br>\n",
    "            <a href=\"https://www.google.com\" class=\"search\" id=\"google\">구글</a> <br>\n",
    "            <a href=\"http://www.daum.net\" class=\"portal\" id=\"daum\">다음</a> <br>\n",
    "            <a href=\"http://www.nl.go.kr\" class=\"government\" id=\"nl\">국립중앙도서관</a> <br>\n",
    "        </body>\n",
    "    </html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 태그안에 속성을 삽입해둠\n",
    "# 태그 안의 속성이 'class'인 경우 '태그.class_속성값'으로 원하는 요소를 추출하는 예이다.\n",
    "f = open('E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\HTML_example_my_site.html', encoding = 'utf-8')\n",
    "\n",
    "html3 = f.read()\n",
    "f.close()\n",
    "\n",
    "soup3 = BeautifulSoup(html3, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"search\" href=\"https://www.google.com\" id=\"google\">구글</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>,\n",
       " <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTML 소스에서 태그가 a이면서 class 속성값이 \"portal\"인 요소만 가져오려면 다음과 같이 수행\n",
    "soup3.select('a.portal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음의 코드는 모두 같은 결과를 보여줍니다(위 예제 참고)\n",
    "```python\n",
    "soup3.select('html body a')\n",
    "soup3.select('body a')\n",
    "soup3.select('html a')\n",
    "soup3.select('a')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 태그를 포함하는 요소 중 id 속성이 \"naver\"인 요소를 선택하려면 다음과 같이 수행됨\n",
    "soup3.select(\"a#naver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\br_example_constitution.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile E:\\Dropbox\\데이터_분석을_위한_파이썬_철저_입문\\chap14\\br_example_constitution.html\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <meta charset=\"utf-8\">\n",
    "            <title>줄 바꿈 테스트 예제</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <p id=\"title\"><b>대한민국헌법</b></p>\n",
    "            <p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
    "            <p id=\"content\">제2조 <br/>①대한민국의 국민이 되는 요건은 법률로 정한다.<br/>②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.</p>\n",
    "        </body>\n",
    "    </html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "제1조 ①대한민국은 민주공화국이다.②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "제2조 ①대한민국의 국민이 되는 요건은 법률로 정한다.②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n"
     ]
    }
   ],
   "source": [
    "# 태그 요소(p 태그, br 태그)에서 텍스트만 추출하여 출력\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 왜 위에는 되고 아래는 안되는지 이해불가\n",
    "f = open('E:/Dropbox/데이터_분석을_위한_파이썬_철저_입문/chap14/br_example_constitution.html', encoding = 'utf-8')\n",
    "\n",
    "html_source = f.read()\n",
    "f.close()\n",
    "\n",
    "soup = BeautifulSoup(html_source, \"lxml\")\n",
    "\n",
    "title = soup.find('p', {\"id\": \"title\"})\n",
    "contents = soup.find_all('p', {\"id\": \"content\"})\n",
    "\n",
    "print(title.get_text())\n",
    "for content in contents:\n",
    "    print(content.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 태그 p로 찾은 요소\n",
      "<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
      "==> 결과에서 태그 br로 찾은 요소: <br/>\n",
      "==> 태그 br을 개행문자로 바꾼 결과\n",
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "# find_result = BeautifulSoup.find('태그')\n",
    "# find_result.replace_with('새 태그나 문자열')\n",
    "\n",
    "html1 = '<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>'\n",
    "\n",
    "soup1 = BeautifulSoup(html1, \"lxml\")\n",
    "\n",
    "print(\"==> 태그 p로 찾은 요소\")\n",
    "content1 = soup1.find('p', {\"id\": \"content\"})\n",
    "print(content1)\n",
    "\n",
    "br_content = content1.find(\"br\")\n",
    "print(\"==> 결과에서 태그 br로 찾은 요소:\", br_content)\n",
    "\n",
    "br_content.replace_with(\"\\n\")\n",
    "print(\"==> 태그 br을 개행문자로 바꾼 결과\")\n",
    "print(content1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출된 br 태그 전체에 적용하려면 다음과 같이 하면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html1, \"lxml\")\n",
    "content2 = soup2.find('p', {\"id\": \"content\"})\n",
    "\n",
    "br_contents = content2.find_all(\"br\")\n",
    "for br_content in br_contents:\n",
    "    br_content.replace_with(\"\\n\")\n",
    "print(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 기능을 함수화 시켜보자\n",
    "def replace_newline(soup_html):\n",
    "    br_to_newlines = soup_html.find_all(\"br\")\n",
    "    for br_to_newline in br_to_newlines:\n",
    "        br_to_newline.replace_with(\"\\n\")\n",
    "    return soup_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n"
     ]
    }
   ],
   "source": [
    "# 함수 적용 예\n",
    "soup2 = BeautifulSoup(html1, \"lxml\")\n",
    "content2 = soup2.find('p', {\"id\": \"content\"})\n",
    "content3 = replace_newline(content2)\n",
    "print(content3.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법 \n",
      "\n",
      "제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다. \n",
      "\n",
      "제2조 \n",
      "①대한민국의 국민이 되는 요건은 법률로 정한다.\n",
      "②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위의 내용 총 정리\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_source, \"lxml\")\n",
    "\n",
    "title = soup.find('p', {\"id\":\"title\"})\n",
    "contents = soup.find_all('p', {\"id\": \"content\"})\n",
    "\n",
    "print(title.get_text(), '\\n')\n",
    "\n",
    "for content in contents:\n",
    "    content1 = replace_newline(content)\n",
    "    print(content1.get_text(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후 실제 웹사이트 스크래핑 진행하면 됨...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 웹 사이트 스크래핑 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alexa.com 에서 실제 웹사이트 이용 순위정보 링크를 스크래핑 해오는 것을 진행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "# p 태그의 요소 안에서 a 태그의 요소를 찾음\n",
    "website_ranking = soup_website_ranking.select('p a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/google.co.kr\">Google.co.kr</a>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this explanation'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사이트 이름 (txt)만 추출\n",
    "website_ranking[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this explanation',\n",
       " 'Naver.com',\n",
       " 'Google.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Google.co.kr']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top Sites in South Korea]\n",
      "1: Naver.com\n",
      "2: Google.com\n",
      "3: Youtube.com\n",
      "4: Daum.net\n",
      "5: Google.co.kr\n",
      "6: Tistory.com\n"
     ]
    }
   ],
   "source": [
    "# 위에서 실행한 코드를 종합해 보면 다음과 같습니다.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "# p 태그 요소 안에서 a 태그의 요소를 찾음\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking]\n",
    "\n",
    "print(\"[Top Sites in South Korea]\")\n",
    "for k in range(1, 7):\n",
    "    print(\"{0}: {1}\".format(k, website_ranking_address[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Google.co.kr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Namu.wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kakao.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Torrentwal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Facebook.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Website\n",
       "1        Naver.com\n",
       "2       Google.com\n",
       "3      Youtube.com\n",
       "4         Daum.net\n",
       "5     Google.co.kr\n",
       "6      Tistory.com\n",
       "7        Namu.wiki\n",
       "8        Kakao.com\n",
       "9   Torrentwal.com\n",
       "10    Facebook.com"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas 패키지를 활용하여 보기 좋게 출력\n",
    "import pandas as pd\n",
    "\n",
    "website_ranking_dict = {'Website': website_ranking_address}\n",
    "# website_ranking_dict\n",
    "df = pd.DataFrame(website_ranking_dict, columns = ['Website'],\n",
    "                  index = range(1, len(website_ranking_address) + 1))\n",
    "df.drop([1], inplace = True)\n",
    "df.index = list(range(1, 51)) # Index 강제 재설정\n",
    "df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**music.naver.com**에서 음악 순위 스크래핑 하는 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey)</span>,\n",
       " <span class=\"ellipsis\">주저하는 연인들을 위해</span>,\n",
       " <span class=\"ellipsis\">FANCY</span>,\n",
       " <span class=\"ellipsis\">나만, 봄</span>,\n",
       " <span class=\"ellipsis\">사계 (Four Seasons)</span>,\n",
       " <span class=\"ellipsis\">Goodbye</span>,\n",
       " <span class=\"ellipsis\">Kill This Love</span>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2019&month=5&week=2\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_title\"인 것을 찾고\n",
    "# 그 안에서 span 태그 요소 중에서 class 속성 값이 \"ellipsis\"인 요소를 추출\n",
    "print(type(html_music))\n",
    "titles = soup_music.select('a._title span.ellipsis')\n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한 페이지에 50개의 순위만 보여주므로 실제 순위는 50위까지만 나옵니다 :)\n",
    "# 페이지 변수를 반복 시키면 Top100도 한꺼번에 스크래핑 가능할 것 같습니다.\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노래 제목만 짤라봅니다.\n",
    "music_titles = [title.get_text() for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey)',\n",
       " '주저하는 연인들을 위해',\n",
       " 'FANCY',\n",
       " '나만, 봄',\n",
       " '사계 (Four Seasons)',\n",
       " 'Goodbye',\n",
       " 'Kill This Love',\n",
       " '뜨거운 여름밤은 가고 남은 건 볼품없지만',\n",
       " '노래방에서',\n",
       " '당신과는 천천히',\n",
       " '달라달라',\n",
       " '소우주 (Mikrokosmos)',\n",
       " '옥탑방 (Rooftop)',\n",
       " '2002',\n",
       " '비올레타',\n",
       " 'BET BET',\n",
       " '사랑하긴 했었나요 스쳐가는 인연이었나요 짧지않은 우리 함께했던 시간들이 자꾸 내 마음을 가둬두네',\n",
       " '그때가 좋았어',\n",
       " '그건 아마 우리의 잘못은 아닐 거야',\n",
       " 'bad guy',\n",
       " '별 보러 갈래?',\n",
       " 'Segno',\n",
       " '너를 만나',\n",
       " 'Make It Right',\n",
       " '벌써 12시',\n",
       " 'She (Hidden Track No.V 1월 선정곡)',\n",
       " 'BASS',\n",
       " '모든 날, 모든 순간 (Every day, Every Moment)',\n",
       " '고고베베 (gogobebe)',\n",
       " 'Talk about love',\n",
       " '신청곡 (Feat. SUGA of BTS)',\n",
       " 'Different',\n",
       " 'Fine',\n",
       " 'Dionysus',\n",
       " '넘쳐흘러',\n",
       " '사월이 지나면 우리 헤어져요 (Beautiful goodbye)',\n",
       " '술이 달다 (Feat. Crush)',\n",
       " '봄날',\n",
       " 'YES or YES',\n",
       " 'HOME',\n",
       " '멍청이(twit)',\n",
       " '나는 볼 수 없던 이야기',\n",
       " '다섯 번째 계절 (SSFWL)',\n",
       " '밤편지',\n",
       " '고백',\n",
       " \"Don't Know What To Do\",\n",
       " 'Blue',\n",
       " '180도',\n",
       " '사계 (하루살이)',\n",
       " 'Jamais Vu']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# music_titles[0:10]\n",
    "music_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'잔나비'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# artist 명 스크래핑 예제입니다\n",
    "artists = soup_music.select('a._artist span.ellipsis')\n",
    "artists[0].get_text()\n",
    "\n",
    "# 개행문자 삭제\n",
    "artists[0].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개를 다 갖고 옵니다.\n",
    "music_artists = [artist.get_text().strip() for artist in artists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['잔나비',\n",
       " 'TWICE(트와이스)',\n",
       " '볼빨간사춘기',\n",
       " '태연 (TAEYEON)',\n",
       " '박효신',\n",
       " 'BLACKPINK',\n",
       " '잔나비',\n",
       " '장범준',\n",
       " '장범준',\n",
       " 'ITZY(있지)']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select 선택자 설정이 일반적이지 않아 일부 아티스트가 누락됩니다...\n",
    "# artist 쪽에 태그가 2종류가 있는 이유 : 솔로가수가 아닌, featuring 고려한 가수 표현 때문\n",
    "# (동적 기능 필요 : js 사용)\n",
    "# 트리구조에서 좀 더 일반적인 태그 구조가 없는지 다시 살펴 봐야 합니다.\n",
    "music_artists[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그를 다시 살펴봅니다.\n",
    "# td 태그 중 class 속성 값이 \"_artist\" 인 것을 찾고\n",
    "# 그 안에서 a 태그의 요소를 추출합니다.\n",
    "artists = soup_music.select('td._artist a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a alt=\"\" class=\"NPI=a:layerbtn,r:1\" href=\"javascript:void(0);\" title=\"\">방탄소년단</a>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'방탄소년단'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아주 잘나옵니다.\n",
    "artists[0].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# music_artists 리스트 재 설정합니다\n",
    "music_artists = [artist.get_text().strip() for artist in artists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트의 형태로 빠짐없이 artist들을 모았습니다. \n",
    "len(music_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey) / 방탄소년단\n",
      "2: 주저하는 연인들을 위해 / 잔나비\n",
      "3: FANCY / TWICE(트와이스)\n",
      "4: 나만, 봄 / 볼빨간사춘기\n",
      "5: 사계 (Four Seasons) / 태연 (TAEYEON)\n",
      "6: Goodbye / 박효신\n",
      "7: Kill This Love / BLACKPINK\n",
      "8: 뜨거운 여름밤은 가고 남은 건 볼품없지만 / 잔나비\n",
      "9: 노래방에서 / 장범준\n",
      "10: 당신과는 천천히 / 장범준\n",
      "11: 달라달라 / ITZY(있지)\n",
      "12: 소우주 (Mikrokosmos) / 방탄소년단\n",
      "13: 옥탑방 (Rooftop) / 엔플라잉(N.Flying)\n",
      "14: 비올레타 / IZ*ONE(아이즈원)\n",
      "15: BET BET / 뉴이스트\n",
      "16: 사랑하긴 했었나요 스쳐가는 인연이었나요 짧지않은 우리 함께했던 시간들이 자꾸 내 마음을 가둬두네 / 잔나비\n",
      "17: 그때가 좋았어 / 케이시(Kassy)\n",
      "18: 그건 아마 우리의 잘못은 아닐 거야 / 백예린\n",
      "19: 별 보러 갈래? / 볼빨간사춘기\n",
      "20: Segno / 뉴이스트\n",
      "21: 너를 만나 / 폴킴\n",
      "22: Make It Right / 방탄소년단\n",
      "23: 벌써 12시 / 청하\n",
      "24: She (Hidden Track No.V 1월 선정곡) / 잔나비\n",
      "25: BASS / 뉴이스트\n",
      "26: 모든 날, 모든 순간 (Every day, Every Moment) / 폴킴\n",
      "27: 고고베베 (gogobebe) / 마마무(Mamamoo)\n",
      "28: Talk about love / 뉴이스트\n",
      "29: 신청곡 (Feat. SUGA of BTS) / 이소라\n",
      "30: Different / 뉴이스트\n",
      "31: Fine / 뉴이스트\n",
      "32: Dionysus / 방탄소년단\n",
      "33: 넘쳐흘러 / M.C THE MAX\n",
      "34: 사월이 지나면 우리 헤어져요 (Beautiful goodbye) / 첸(CHEN)\n",
      "35: 술이 달다 (Feat. Crush) / 에픽하이 (EPIK HIGH)\n",
      "36: 봄날 / 방탄소년단\n",
      "37: YES or YES / TWICE(트와이스)\n",
      "38: HOME / 방탄소년단\n",
      "39: 멍청이(twit) / 화사(Hwa Sa)\n",
      "40: 나는 볼 수 없던 이야기 / 잔나비\n",
      "41: 다섯 번째 계절 (SSFWL) / 오마이걸(OH MY GIRL)\n",
      "42: 밤편지 / 아이유(IU)\n",
      "43: 고백 / 양다일\n",
      "44: Don't Know What To Do / BLACKPINK\n",
      "45: Blue / 태연 (TAEYEON)\n",
      "46: 180도 / 벤\n",
      "47: 사계 (하루살이) / M.C THE MAX\n",
      "48: Jamais Vu / 방탄소년단\n",
      "49: 이 노래가 클럽에서 나온다면 / 우디 (Woody)\n",
      "50: 초록빛 / 폴킴\n",
      "51: Way Back Home / 숀(SHAUN)\n",
      "52: 여행 / 볼빨간사춘기\n",
      "53: 나의 기쁨 나의 노래 (Intro) / 잔나비\n",
      "54: IDOL / 방탄소년단\n",
      "55: 신용재 / 하은(라코스테남)\n",
      "56: 열애중 / 벤\n",
      "57: 사랑에 연습이 있었다면 (Prod. 2soo) / 임재현\n",
      "58: 지나오다 / 닐로(Nilo)\n",
      "59: 진심이 담긴 노래 (True Song) / 케이시(Kassy)\n",
      "60: GO HIGH (Feat. 우원재, 창모 (CHANGMO), The Quiett) (Prod. CODE KUNST) / 이영지\n",
      "61: Dance The Night Away / TWICE(트와이스)\n",
      "62: Mermaid / 볼빨간사춘기\n",
      "63: 이 밤 / 양다일\n",
      "64: 봄 (Feat. 산다라박) / 박봄\n",
      "65: SOLO / 제니 (JENNIE)\n",
      "66: 우주선 / 정승환\n",
      "67: 너도 그냥 날 놓아주면 돼 / 윤건\n",
      "68: Intro : Persona / 방탄소년단\n",
      "69: 띵 (Prod. By 기리보이) / Jvcki Wai\n",
      "70: 눈 (Prod. 기리보이) / 강현준 (Lil ..\n",
      "71: 투게더! / 잔나비\n",
      "72: 노래 제목 / 뉴이스트\n",
      "73: November Rain / 잔나비\n",
      "74: 선물 / 멜로망스(Melomance)\n",
      "75: SHE'S FINE / 헤이즈 (Heize)\n",
      "76: 뿜뿜 / 모모랜드(MOMOLAND)\n",
      "77: Universe (별의 언어) / 민현 (뉴이스트)\n",
      "78: MILLIONS / WINNER\n",
      "79: 뚜두뚜두 (DDU-DU DDU-DU) / BLACKPINK\n",
      "80: 첫눈처럼 너에게 가겠다 / 에일리\n",
      "81: 라비앙로즈 (La Vie en Rose) / IZ*ONE(아이즈원)\n",
      "82: FAKE LOVE / 방탄소년단\n",
      "83: 삐삐 / 아이유(IU)\n",
      "84: 비 / 폴킴\n",
      "85: OKGO (Feat. E SENS) / 빈지노 (Beenzino)\n",
      "86: 봄 사랑 벚꽃 말고 / 하이포(HIGH4..\n",
      "87: 그날처럼 / 장덕철\n",
      "88: 일산으로 / 장범준\n",
      "89: 하루도 그대를 사랑하지 않은 적이 없었다 / 임창정\n",
      "90: 내 생에 아름다운 / 케이윌\n",
      "91: 가을 타나 봐 / 바이브\n",
      "92: HELP ME / 뉴이스트 W\n",
      "93: 여보세요 / 뉴이스트\n",
      "94: 흔한 이별 / 허각\n",
      "95: 해야 (Sunrise) / 여자친구(GFRIEND)\n",
      "96: 나들이 갈까 / 볼빨간사춘기\n",
      "97: Dejavu / 뉴이스트 W\n",
      "98: 벚꽃 엔딩 / 버스커 버스커\n",
      "99: 오늘 밤에 / 홍진영\n",
      "100: 전설 / 잔나비\n"
     ]
    }
   ],
   "source": [
    "# 위의 결과들을 종합하여 music.naver.com에서 음악순위를 스크래핑 합니다(1-100위)\n",
    "# 어떤 날의 어떤 랭크를 가져올지를 원하는 대로 선택할 수 있도록 변수화 한다면 실시간 랭킹\n",
    "# 스크래핑도 가능합니다(url항목에 날짜를 삽입).\n",
    "import calendar\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 랭킹(1-100까지) 다 가져옵니다\n",
    "total_music_titles = list()\n",
    "total_music_artists = list()\n",
    "\n",
    "# 실시간 날짜 계산\n",
    "# year, month : 그냥 가져온다\n",
    "today = date.today()\n",
    "year = today.year\n",
    "month = today.month\n",
    "# # week : 일단 달력을 끄집어 와서 첫째주 둘째주 세째주를 그때그때마다 정의를 한다\n",
    "# # 참고 링크 : https://stackoverflow.com/questions/53583322/get-the-start-and-end-date-of-all-the-weeks-for-a-given-month-in-python-exclusiv/53583540\n",
    "# 위의 방법보다, 실제 웹페이지에서 정의한 날짜들을 스크래핑해서 가져오는걸로 로직을 수정합니다.\n",
    "# (추후 추가시켜 실시간 or 선택한 날짜로 스크래핑한 TOP 100을 보일 수 있도록 합니다)\n",
    "\n",
    "for i in range(1, 3):\n",
    "    url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=\"+str(year)+\"&month=\"+str(month)+\"&week=1&page=\"+str(i)\n",
    "    html_music = requests.get(url).text\n",
    "    soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "    titles = soup_music.select('a._title span.ellipsis')\n",
    "    artists = soup_music.select('td._artist a')\n",
    "\n",
    "    music_titles = [title.get_text() for title in titles]\n",
    "    music_artists = [artist.get_text().strip() for artist in artists]\n",
    "    \n",
    "    total_music_titles += music_titles\n",
    "    total_music_artists += music_artists\n",
    "\n",
    "for k in range(100):\n",
    "    print(\"{0}: {1} / {2}\".format(k + 1, total_music_titles[k], total_music_artists[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-28 - 2019-05-04\n",
      "2019-05-05 - 2019-05-11\n",
      "2019-05-12 - 2019-05-18\n",
      "2019-05-19 - 2019-05-25\n",
      "2019-05-26 - 2019-06-01\n"
     ]
    }
   ],
   "source": [
    "# Week order 계산 연습 예제\n",
    "# 참고 링크 : https://stackoverflow.com/questions/53583322/get-the-start-and-end-date-of-all-the-weeks-for-a-given-month-in-python-exclusiv/53583540\n",
    "import datetime\n",
    "import calendar as cd\n",
    "cld = cd.Calendar(firstweekday = 0)\n",
    "for end_day in cld.itermonthdates(2019, 5):\n",
    "#     print(end_day)\n",
    "    if end_day.weekday() == 5:\n",
    "        start_day = end_day - datetime.timedelta(6)\n",
    "        print(\"{} - {}\".format(start_day.isoformat(), end_day.isoformat()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곡명과 아티스트를 하나의 변수로(Dictionary 이용)관리합니다.\n",
    "# (위 소스를 기준으로 수정)\n",
    "music_titles_artists = {}\n",
    "order = 0\n",
    "\n",
    "for (music_title, music_artist) in zip(total_music_titles, total_music_artists):\n",
    "    order += 1\n",
    "    music_titles_artists[order] = [music_title, music_artist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey)', '방탄소년단']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary key(순위가 곧 key)로 접근 가능합니다!\n",
    "music_titles_artists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['주저하는 연인들을 위해', '잔나비']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles_artists[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./naver_music_scrapping_data/NaverMusicTop100_2019-05-14.txt']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스크래핑 기능 함수화, 스크래핑 결과를 file 형태로 저장하는 파이썬 소스코드 입니다.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, timedelta\n",
    "import glob\n",
    "\n",
    "# 날짜 부분 미리 계산\n",
    "today = date.today()\n",
    "year = today.year\n",
    "month = today.month\n",
    "\n",
    "naver_music_url = \"https://music.naver.com/listen/history/index.nhn?type=TOTAL_V2&year=\"+str(year)+\"&month=\"+str(month)+\"&week=1&page=\"\n",
    "\n",
    "# naver music 페이지에서 랭킹 정보 스크래핑 하는 함수\n",
    "def naver_music_scrapping(url):\n",
    "    html_music = requests.get(url).text\n",
    "    soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "    titles = soup_music.select('a._title span.ellipsis')\n",
    "    artists = soup_music.select('td._artist a')\n",
    "\n",
    "    music_titles = [title.get_text() for title in titles]\n",
    "    music_artists = [artist.get_text().strip() for artist in artists]\n",
    "    \n",
    "    return music_titles, music_artists\n",
    "\n",
    "# 노래 제목과 아티스트를 저장할 파일 이름을 폴더와 함께 저장\n",
    "file_name = \"./naver_music_scrapping_data/NaverMusicTop100_\"+str(date.today())+\".txt\"\n",
    "\n",
    "# 파일을 읽고 씁니다.(with 구문 활용)\n",
    "with open(file_name, \"w\") as f:\n",
    "    for page in range(2):\n",
    "        naver_music_url_page = naver_music_url + str(page + 1) # page URL\n",
    "        naver_music_titles, naver_music_artists = naver_music_scrapping(naver_music_url_page)\n",
    "        \n",
    "        # 추출된 노래 제목과 아티스트를 파일에 저장\n",
    "        for i in range(len(naver_music_titles)):\n",
    "            f.write(\"{0:2d}: {1} / {2} \\n\".format(page*50 + i + 1, naver_music_titles[i], naver_music_artists[i]))\n",
    "\n",
    "glob.glob(file_name) # 생성된 파일 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 페이지에서 이미지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "html_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo.png'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_file_name = os.path.basename(url)\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(folder)\n",
    "# os.path.exists(folder)\n",
    "folder = './download'\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./download\\\\python-logo.png'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.path.join(path1[,path2[,...]])\n",
    "# path1과 path2를 합쳐 새로운 경로를 생성\n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일 저장 코드\n",
    "with open(image_path, 'wb') as imageFile:\n",
    "    chunk_size = 1000000\n",
    "    for chunk in html_image.iter_content(chunk_size):\n",
    "        imageFile.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python-logo.png']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 코드 통합\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "image_file_name = os.path.basename(url)\n",
    "\n",
    "folder = './download'\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "\n",
    "# 이미지 파일 저장 코드\n",
    "with open(image_path, 'wb') as imageFile:\n",
    "    chunk_size = 1000000\n",
    "    for chunk in html_image.iter_content(chunk_size):\n",
    "        imageFile.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 이미지 내려 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"\" src=\"https://cdn.pixabay.com/photo/2019/05/05/18/58/girl-4181395__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2019/05/05/18/58/girl-4181395__340.jpg 1x, https://cdn.pixabay.com/photo/2019/05/05/18/58/girl-4181395__480.jpg 2x\"/>,\n",
       " <img alt=\"\" src=\"https://cdn.pixabay.com/photo/2019/05/07/16/19/strawberry-4186310__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2019/05/07/16/19/strawberry-4186310__340.jpg 1x, https://cdn.pixabay.com/photo/2019/05/07/16/19/strawberry-4186310__480.jpg 2x\"/>,\n",
       " <img alt=\"\" src=\"https://cdn.pixabay.com/photo/2018/11/22/18/06/desert-3832488__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2018/11/22/18/06/desert-3832488__340.jpg 1x, https://cdn.pixabay.com/photo/2018/11/22/18/06/desert-3832488__480.jpg 2x\"/>]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://pixabay.com/ko/photos/?order=popular&cat=animals'\n",
    "\n",
    "html_pixabay_image = requests.get(URL).text\n",
    "soup_pixabay_image = BeautifulSoup(html_pixabay_image, \"lxml\")\n",
    "pixabay_image_elements = soup_pixabay_image.select('img')\n",
    "pixabay_image_elements[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.pixabay.com/photo/2019/05/05/18/58/girl-4181395__340.jpg'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BeautifulSoup에서 get함수를 이용, 속성값을 반환\n",
    "# src 속성값인 이미지 주소를 구하려면 다음 코드처럼 get('src')을 수행하면 됨.\n",
    "pixabay_image_url = pixabay_image_elements[0].get('src')\n",
    "pixabay_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 개념을 활용하여 이미지를 다운받는 코드 작성\n",
    "html_image = requests.get(pixabay_image_url)\n",
    "\n",
    "folder = \"./download\"\n",
    "\n",
    "with open(os.path.join(folder, os.path.basename(pixabay_image_url)), 'wb') as imageFile:\n",
    "    chunk_size = 1000000\n",
    "    for chunk in html_image.iter_content(chunk_size):\n",
    "        imageFile.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일명: 'girl-4181395__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'strawberry-4186310__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'desert-3832488__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'man-4191143__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'chicago-4140659__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'dandelion-4186029__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'iceland-219182__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'ramadan-4159961__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'food-4185324__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'lilac-4193079__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'girl-4185917__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'soap-601239__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'trees-sunrise-3796183__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'trains-4184537__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'bulldog-4187023__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'travel-4191991__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'oldtimer-4190813__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'sky-823624__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'lantern-3713493__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'purple-4163951__340.jpg'. 내려받기 완료!\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '/static/img/blank.gif': No schema supplied. Perhaps you meant http:///static/img/blank.gif?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-f27c9feec4cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_of_download_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mdownload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigure_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpixabay_image_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"선택한 모든 이미지 내려받기 완료!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-186-f27c9feec4cc>\u001b[0m in \u001b[0;36mdownload_image\u001b[1;34m(img_folder, img_url)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdownload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mhtml_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m# os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimageFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\smart\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\smart\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\smart\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         )\n\u001b[1;32m--> 519\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\smart\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m         )\n\u001b[0;32m    464\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\smart\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\smart\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '/static/img/blank.gif': No schema supplied. Perhaps you meant http:///static/img/blank.gif?"
     ]
    }
   ],
   "source": [
    "# 위의 내용을 총 활용하여 여러개의 이미지를 한꺼번에 받는 소스코드 작성\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# URL(주소)에서 이미지 주소 추출 함수\n",
    "def get_image_url(url):\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")\n",
    "    image_elements = soup_image_url.select('img')\n",
    "    \n",
    "    if (image_elements != None):\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 폴더를 지정해 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if (img_url != None):\n",
    "        html_image = requests.get(img_url)\n",
    "        # os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리\n",
    "        with open(os.path.join(img_folder, os.path.basename(img_url)), 'wb') as imageFile:\n",
    "            chunk_size = 1000000 # 이미지 데이터를 1000000바이트씩 나눠 저장\n",
    "            for chunk in html_image.iter_content(chunk_size):\n",
    "                imageFile.write(chunk)        \n",
    "        print(\"이미지 파일명: '{0}'. 내려받기 완료!\".format(os.path.basename(img_url)))\n",
    "    else:\n",
    "        print(\"내려받을 이미지가 없습니다.\")\n",
    "    \n",
    "# 웹 사이트의 주소 지정\n",
    "pixabay_url = \"https://pixabay.com/ko/photos/?order=popular&cat=animals\"\n",
    "\n",
    "figure_folder = \"./download\"\n",
    "\n",
    "pixabay_image_urls = get_image_url(pixabay_url) # 이미지 파일의 주소 가져오기\n",
    "\n",
    "# num_of_download_image = 7 # 내려받을 이미지 개수 결정\n",
    "num_of_download_image = len(pixabay_image_urls)\n",
    "\n",
    "for i in range(num_of_download_image):\n",
    "    download_image(figure_folder, pixabay_image_urls[i])\n",
    "print(\"=\"*50)\n",
    "print(\"선택한 모든 이미지 내려받기 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_download_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
